{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdolfoBanchio/Redes-neuronales-2024/blob/main/%20%20%20%20/guias_ejercicios/redes_neuronales_2024_guia_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: Aprendiendo Fashion-MNIST\n",
        "\n",
        "## Refs.\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "\n",
        "* https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "* https://github.com/pranay414/Fashion-MNIST-Pytorch/blob/master/fashion_mnist.ipynb\n",
        "\n",
        "## **Ejercicio 1)** Importando librerías\n",
        "\n",
        "**0)** De ser necesario, **instale PyTorch** escribiendo\n",
        "\n",
        "    !pip3 install torch torchvision torchaudio torchviz\n",
        "\n",
        "**1)** Importe las librerías estandard de Python: `os`, `datetime`, `collections` y `pickle`.\n",
        "\n",
        "**2)** Importe las siguientes librerías third party de Python: `matplotlib.pyplot`, `numpy`, `scipy`, `sklearn`, `pandas`, `dill` y `json`.\n",
        "\n",
        "**3)** Importe las librerias necesarias de **PyTorch**: `torch` y `torchvision`.\n",
        "\n",
        "**4)** Importe la librería: `google.colab`."
      ],
      "metadata": {
        "id": "NRYEofSD0xoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.0)\n",
        "!pip3 install torch torchvision torchaudio torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D79v2F3-s7vu",
        "outputId": "bcaebcdc-1d36-41ef-f4a2-e34e697793e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=34f40f88746fe18ea078f6fbf0a9b9cef86f14e477a64f96d1319944d613676a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1)\n",
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "I8N3D_nU1_oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2)\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.linalg as linalg\n",
        "import sklearn as skl\n",
        "import pandas as pd\n",
        "#import dill\n",
        "import json"
      ],
      "metadata": {
        "id": "QsfFvPYhkCGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3)\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "#from torchviz import make_dot"
      ],
      "metadata": {
        "id": "Uot5sVNnkCNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.4)\n",
        "import google.colab\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "rVCiYt-1kCUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 2)**\n",
        "\n",
        "Bajando y Jugando con el dataset **Fashion-MNIST**.\n",
        "\n",
        "**1)** Baje y transforme (i.e. normalize los valores al rango [0,1]) los conjuntos de entrenamiento y testeo de FashionMNIST.\n",
        "\n",
        "**2)** Explore algunos ejemplos de estos conjuntos. Que formato poseen?\n",
        "\n",
        "**3)** Visitando la página web de FashionMNIST, cree un diccionario de Python `Dict()` asociando cada categoría a un nombre adecuado de la misma.\n",
        "\n",
        "**4)** Grafique un mosaico de 3x3 imagenes de FashionMNIST, cada una titulada con su respectiva clasificación"
      ],
      "metadata": {
        "id": "NcaGEHAd10sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1)"
      ],
      "metadata": {
        "id": "NUoQ9bnwaZ7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12075a20-655e-4309-8cf0-16bedff27786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:03<00:00, 8.54MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 134kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.56MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 14.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 3)\n",
        "\n",
        "Creando un `DataLoader` para alimentar el modelo con batchs (lotes) de entrenamiento.\n",
        "\n",
        "**1)** Cree los `DataLoader`s para cada conjunto. Defínalos con un `batch_size` de 100 y con el flag `shuffle` seteado a `True`.\n",
        "\n",
        "**2)** Use uno de los `DataLoader`s creados anteriormente para explorar algunos elementos del conjunto.\n",
        "\n",
        "Notar que, el iterador devuelve el batch en un par `(image,label)`.\n",
        "\n",
        "El objeto `images` es un tensor de dimensiones `(100,1,28,28)`.\n",
        "El 100 es el tamaño del batch.\n",
        "El 1 porque hay un solo canal (en este caso, un canal de escala de grises, pero podría haber varios, p. ej. uno por cada color de {Red, Green Blue} en caso que fuesen imagenes a color).\n",
        "Luego, 28 y 28 porque cada imagen del dataset es de 28 x 28 píxeles.\n",
        "\n",
        "El objeto `labels` es un tensor de dimensiones `(100,)`.\n",
        "La $i$-ésima entrada `labels[i]` de `labels` es un número en $\\{0,1,...,9\\}$ indicando la categoría a la que pertenece la $i$-ésima imagen en el batch, guardada en `images[i]`."
      ],
      "metadata": {
        "id": "1OWYnfxWz8RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1)"
      ],
      "metadata": {
        "id": "lK7gqh7lrTi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 4)\n",
        "\n",
        "Defina una red neuronal de 4 capas, una de entrada, dos ocultas de $n_1=128$ y $n_2=64$ neuronas, respectivamente, y una de salida de 10 neuronas.\n",
        "\n",
        "En las capas intermedias utilice neuronas tipo ReLU y agregueles un *dropout* de p=0.2.\n",
        "En la capa de salida no utilice funciones de activación ni dropout.\n",
        "\n",
        "Las capas sucesivas tienen que estar totalmente conectadas entre si."
      ],
      "metadata": {
        "id": "REToccG127zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4)"
      ],
      "metadata": {
        "id": "C17Tib9G_k-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 5)\n",
        "\n",
        "Entrenamos el modelo\n",
        "\n",
        "**1)** Implemente, en una función, un loop de entrenamiento que recorra los batchs (lotes).\n",
        "\n",
        "**2)** Implemente, en una función, un loop de validación que recorra los batchs.\n",
        "\n",
        "**3)** Inicialize dos `DataLoader`s llamados `train_loader` y `valid_loader` a partir del `train_set` (conjunto de entranmiento) y del `valid_set` (conjunto de validación) de Fashion-MNIST, respectivamente, y que usen batchs de 100 ejemplos.\n",
        "\n",
        "**4)** Cree una función de pérdida usando la **Cross Entropy Loss**.\n",
        "\n",
        "**IMPORTANTE:** Notar que la **Cross Entropy Loss** aplica automáticamente una `log_softmax`.\n",
        "\n",
        "**5)** Cree un optimizador que utilice el método de **Stochastic Gradient Descent** con un learning rate igual a $10^{-3}$.\n",
        "\n",
        "**6)** Cree una instancia del modelo.\n",
        "\n",
        "**7)** Especifique en que dispositivo (`device`) va a trabajar: en una **CPU** o en una **GPU**.\n",
        "\n",
        "**8)** Implemente un loop de entrenamiento y validación que trabaje con el `train_loader` y el `valid_loader`, respectivamente, usando un numero arbitrario de épocas.\n",
        "Este loop debe guardar en cuatro listas los valores de los promedios del **Cross Entropy Loss** y las fracciones de clasificaciones correctas o **precisión** (accuracy) sobre el conjunto de **entrenamiento** y el de **validación**, respectivamente.\n",
        "\n",
        "**IMPORTANTE:** No olvide copiar los batchs al dispositivo de trabajo.\n",
        "\n",
        "**9)** Entrene y valide el modelo.\n",
        "\n",
        "**10)** Use las listas del inciso anterior para graficar en función de las épocas la **Cross Entropy Loss** de **entrenamiento** y de **validación**.\n",
        "Realize un gráfico análogo pero con la **precisión**.\n",
        "Discuta y comente, cual es el número óptimo de épocas de entrenamiento?\n",
        "\n",
        "**11)** Repita los experimentos variando hiperparámetros. Por ejemplo:\n",
        "\n",
        "- El learning-rate.\n",
        "- El optimizador (ej. puede usar ADAM).\n",
        "- El valor de dropout.\n",
        "- El número de neuronas en las capas intermedias.\n",
        "- El número de épocas de entrenamiento.\n",
        "- El tamaño de los lotes.\n",
        "\n",
        "Discuta los resultados."
      ],
      "metadata": {
        "id": "A9uINlg69OTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1)"
      ],
      "metadata": {
        "id": "hyuXv-0x29Xw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}